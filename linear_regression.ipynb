{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee9ffef",
   "metadata": {},
   "source": [
    "LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b0def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (100,)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(100) * 10  # feature values between 0 and 10\n",
    "y = 3.5 * X + 4 + np.random.randn(100) * 2  # linear relation with noise\n",
    "\n",
    "print(X.shape, y.shape)  # (100,) (100,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496a98e",
   "metadata": {},
   "source": [
    "# Gradient Descent with Mean Squared Error (MSE)\n",
    "\n",
    "The **Mean Squared Error (MSE)** is a common cost function for regression problems.  \n",
    "It measures the average squared difference between predicted values \\( \\hat{y} \\) and actual values \\( y \\).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Cost Function\n",
    "\n",
    "For \\( m \\) training examples:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\hat{y}^{(i)} = w^T x^{(i)} + b \\) is the prediction for the \\(i\\)-th example\n",
    "- \\( y^{(i)} \\) is the true target value\n",
    "- \\( w \\) is the weight vector\n",
    "- \\( b \\) is the bias (intercept term)\n",
    "\n",
    "The factor \\( \\frac{1}{2} \\) is included to simplify the derivative.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Gradients\n",
    "\n",
    "The partial derivatives of \\( J(w,b) \\) are:\n",
    "\n",
    "For weights \\( w \\):\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right) x^{(i)}\n",
    "$$\n",
    "\n",
    "For bias \\( b \\):\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Gradient Descent Updates\n",
    "\n",
    "Simultaneously update \\( w \\) and \\( b \\):\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right) x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\alpha \\) = learning rate (step size)\n",
    "- \\( m \\) = number of training examples\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Vectorized Form\n",
    "\n",
    "Let:\n",
    "- \\( X \\) = \\( m \\times n \\) feature matrix\n",
    "- \\( y \\) = \\( m \\times 1 \\) target vector\n",
    "- \\( w \\) = \\( n \\times 1 \\) weight vector\n",
    "- \\( \\mathbf{1} \\) = \\( m \\times 1 \\) column vector of ones\n",
    "\n",
    "Predictions:\n",
    "$$\n",
    "\\hat{y} = Xw + b\\mathbf{1}\n",
    "$$\n",
    "\n",
    "Updates:\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{1}{m} X^T (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{1}{m} \\mathbf{1}^T (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Notes\n",
    "- MSE is convex for linear regression → gradient descent is guaranteed to find the global minimum.\n",
    "- Feature scaling (normalization) can significantly speed up convergence.\n",
    "- Learning rate \\( \\alpha \\) must be tuned for stability and speed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65712629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(w, b,x):\n",
    "\n",
    "    m = x.shape[0]\n",
    "    f_wb = np.zeros(m)\n",
    "    for i in range(m):\n",
    "         f_wb[i] = x[i]*w + b\n",
    "    \n",
    "    return f_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bccfc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    cost_sum = 0\n",
    "    for i in range(m):\n",
    "        f_wb = x[i] * w + b     # prediction for i-th example\n",
    "        cost = (f_wb - y[i]) ** 2   # no indexing into f_wb again!\n",
    "        cost_sum += cost\n",
    "    mse = (1 / (2 * m)) * cost_sum\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb92ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(x, y, w, b):\n",
    "    y_pred = x * w + b\n",
    "    ss_res = np.sum((y - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee754a5",
   "metadata": {},
   "source": [
    "# Gradient Descent Explanation\n",
    "\n",
    "Gradient Descent is a **first-order optimization algorithm** used to minimize a cost function \\( J(w, b) \\) by iteratively updating the parameters (weights and bias) of a model.  \n",
    "The key idea is to **adjust the parameters in the direction that reduces the cost the most** — this is the **negative gradient direction**.\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Initialize parameters** \\( w \\) (weights) and \\( b \\) (bias) — usually with zeros or small random values.\n",
    "2. **Compute predictions** \\( \\hat{y}^{(i)} \\) for each training example \\( x^{(i)} \\) using the hypothesis:\n",
    "   $$\n",
    "   \\hat{y}^{(i)} = h_{w,b}(x^{(i)})\n",
    "   $$\n",
    "   For linear regression:\n",
    "   $$\n",
    "   h_{w,b}(x^{(i)}) = w^T x^{(i)} + b\n",
    "   $$\n",
    "3. **Evaluate the cost function** \\( J(w,b) \\), e.g. Mean Squared Error:\n",
    "   $$\n",
    "   J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2\n",
    "   $$\n",
    "4. **Compute gradients** (partial derivatives) of \\( J \\) with respect to \\( w \\) and \\( b \\):\n",
    "   $$\n",
    "   \\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right) x^{(i)}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right)\n",
    "   $$\n",
    "5. **Update parameters** simultaneously:\n",
    "   $$\n",
    "   w := w - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right) x^{(i)}\n",
    "   $$\n",
    "   $$\n",
    "   b := b - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right)\n",
    "   $$\n",
    "   where:\n",
    "   - \\( \\alpha \\) = learning rate (controls step size)\n",
    "   - \\( m \\) = number of training examples\n",
    "6. **Repeat** steps 2–5 until convergence (when \\( J(w,b) \\) changes very little).\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition\n",
    "\n",
    "- The gradient tells us the slope of the cost function at the current parameters.\n",
    "- If the slope is **positive**, we decrease the parameter.\n",
    "- If the slope is **negative**, we increase the parameter.\n",
    "- By repeating this process, we \"walk downhill\" on the cost function surface until we reach a minimum.\n",
    "\n",
    "---\n",
    "\n",
    "## Vectorized Form (Efficient Computation)\n",
    "\n",
    "Let:\n",
    "- \\( X \\) = \\( m \\times n \\) feature matrix\n",
    "- \\( y \\) = \\( m \\times 1 \\) target vector\n",
    "- \\( w \\) = \\( n \\times 1 \\) weight vector\n",
    "\n",
    "Then the gradient descent update can be written compactly as:\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{1}{m} X^T (Xw + b - y)\n",
    "$$\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{1}{m} \\mathbf{1}^T (Xw + b - y)\n",
    "$$\n",
    "where \\( \\mathbf{1} \\) is an \\( m \\times 1 \\) vector of ones.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Notes\n",
    "- **Learning rate (\\(\\alpha\\))** must be chosen carefully:\n",
    "  - Too small → slow convergence\n",
    "  - Too large → divergence or oscillations\n",
    "- Gradient Descent can be:\n",
    "  - **Batch**: uses all \\( m \\) examples per update.\n",
    "  - **Stochastic**: updates using 1 example at a time.\n",
    "  - **Mini-batch**: uses a small subset per update (common in deep learning).\n",
    "- The algorithm **finds a local minimum** — for convex cost functions, this is the global minimum.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b36711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(X,y, w, b):\n",
    "\n",
    "    m = X.shape[0] #total number of inputs in X\n",
    "\n",
    "    dj_db = 0.0 #this will initialize our gradient for our BIAS\n",
    "    dj_dw = 0.0 #this will initialize our gradient for our WEIGHT\n",
    "    \n",
    "    for i in range(m): #we will take the range of total inputs\n",
    "        f_wb = X[i]*w + b #These will be our predictions\n",
    "        dj_dw_i = (f_wb - y[i]) * X[i] #This will subtract our prediction from actual values to give us the error and multiply it by inputs\n",
    "        dj_db_i = (f_wb - y[i]) #This will only give us the error for our predictions\n",
    "\n",
    "        dj_db += dj_db_i #this will add this to our original bias\n",
    "        dj_dw += dj_dw_i #this will add this to our original weight\n",
    "\n",
    "    dj_dw = dj_dw/m #Finally we will divide both of them by total number of inputs to give us the updated parameters\n",
    "    dj_db = dj_db/m\n",
    "\n",
    "    return dj_db, dj_dw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4b326ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_init, b_init, epochs=500, L_rate=0.03):\n",
    "    # Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    # num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    # Args:\n",
    "    #   x (ndarray (m,))  : Data, m examples \n",
    "    #   y (ndarray (m,))  : target values\n",
    "    #   w_in,b_in (scalar): initial values of model parameters  \n",
    "    #   alpha (float):     Learning rate\n",
    "    #   num_iters (int):   number of iterations to run gradient descent\n",
    "    #   cost_function:     function to call to produce cost\n",
    "    #   gradient_function: function to call to produce gradient\n",
    "      \n",
    "    # Returns:\n",
    "    #   w (scalar): Updated value of parameter after running gradient descent\n",
    "    #   b (scalar): Updted value of parameter after running gradient descent\n",
    "    #   J_history (List): History of cost values\n",
    "    #   p_history (list): History of parameters [w,b] \n",
    "    #   \"\"\"\n",
    "\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "\n",
    "    for i in range(epochs):\n",
    "        db, dw = gradients(X, y, w, b)\n",
    "\n",
    "        w -= dw * L_rate\n",
    "        b -= db * L_rate\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646e2a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.4405231275380768\n"
     ]
    }
   ],
   "source": [
    "w = 100\n",
    "b = 100\n",
    "\n",
    "w, b = gradient_descent(X, y, w, b)\n",
    "\n",
    "mse = mean_squared_error(X,y,  w, b)\n",
    "print(f'MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13eaeaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.9517389594851597\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(X, y, w, b)\n",
    "print(\"R²:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
